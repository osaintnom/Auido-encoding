{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUZxclEkTOcS"
      },
      "source": [
        "## Inicializacion e importe de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hICEx-EqQxWy",
        "outputId": "c9290cca-64c1-4157-e6c5-38053222c564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz8wyXsMQuBE",
        "outputId": "290f4585-257f-4551-e8c4-e4924a0b27f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: torchview in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.35.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu118)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install pydub scikit-learn wandb torchview torchviz graphviz matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dij9PodZQyeA",
        "outputId": "6a80ccc2-1cbf-4343-aefa-f22b057cbb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.35.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZSdBL2673KUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4f6575-bd9e-4967-d656-868109a503e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosaintnom\u001b[0m (\u001b[33mchicas_superpoderosas\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio\n",
        "!pip install  pydub\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchaudio.datasets import GTZAN\n",
        "from torch.utils.data import DataLoader\n",
        "import torchaudio.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uyYQX44SlKF"
      },
      "source": [
        "bfbad9b2649155692b5f97a49a43c0eeb66dff4a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NMyj1ug7Q8gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae0b9c1-3f6b-4cb8-a7bb-53ce07e314cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "device = torch.device(\n",
        "    'mps:0' if torch.backends.mps.is_available() else 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAR3tiGci2-e"
      },
      "source": [
        "\n",
        "# TP4: Encodeador de música\n",
        "\n",
        "\n",
        "\n",
        "## Orden de pasos\n",
        "\n",
        "0. Elijan GPU para que corra mas rapido (RAM --> change runtime type --> T4 GPU)\n",
        "1. Descargamos el dataset y lo descomprimimos en alguna carpeta en nuestro drive.\n",
        "2. Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos.\n",
        "3. Visualización de los archivos\n",
        "4. Clasificación\n",
        "5. Evaluación\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qApAl-j1TbdJ"
      },
      "source": [
        "## BASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rt4FEe853KUX"
      },
      "outputs": [],
      "source": [
        "project_name='Music_genre_classification'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5G8mTE-5zM"
      },
      "source": [
        "### 2. Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "q5AUydgIxfwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8131492a-b012-4e36-c7f9-bf7ee6e0dd44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYLOe3isiV0b"
      },
      "source": [
        "data_dir es el path donde pusimos la carpeta genres. \"'//content/drive/MyDrive/Materias/TD6 - Inteligencia Artificial/TPs/2023/TP4/genres/'\" es un ejemplo. Modificar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "7kYMlPdYrzCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d086dbc4-775b-4ef4-8350-946742a673ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rock',\n",
              " 'pop',\n",
              " 'hiphop',\n",
              " 'disco',\n",
              " 'blues',\n",
              " 'reggae',\n",
              " 'country',\n",
              " 'metal',\n",
              " 'classical',\n",
              " 'jazz']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import os\n",
        "data_dir='/content/drive/MyDrive/genres_5sec/'\n",
        "list_files=os.listdir(data_dir)\n",
        "classes=[]\n",
        "for file in list_files:\n",
        "  name='{}/{}'.format(data_dir,file)\n",
        "  if os.path.isdir(name):\n",
        "    classes.append(file)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "GJxZV04XZtnP"
      },
      "outputs": [],
      "source": [
        "samplerate=22050\n",
        "def parse_genres(fname):\n",
        "    parts = fname.split('/')[-1].split('.')[0]\n",
        "    return parts #' '.join(parts[0])\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.files =[]\n",
        "        for c in classes:\n",
        "          self.files = self.files + [fname for fname in os.listdir(os.path.join(root,c)) if fname.endswith('.wav')]\n",
        "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
        "        #self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "            fname = self.files[i]\n",
        "\n",
        "            #img = self.transform(open_image(fpath))\n",
        "            genre = parse_genres(fname)\n",
        "            fpath = os.path.join(self.root,genre, fname)\n",
        "            genre_index = self.classes.index(genre)\n",
        "            audio = torchaudio.load(fpath)[0]\n",
        "\n",
        "            spectogram = tt.Spectrogram(\n",
        "                n_fft=1024,\n",
        "            )(audio)\n",
        "\n",
        "            # MelSpectrogram tt.MelSpectrogram(sample_rate=samplerate, n_fft=1024, hop_length=512, n_mels=128)(audio)\n",
        "            # mel_spectogram = tt.MelSpectrogram(sample_rate=samplerate, n_fft=1024, hop_length=200, n_mels=201)(audio)\n",
        "\n",
        "            hop_length=432\n",
        "            n_mels=256\n",
        "            n_fft = 2*(n_mels - 1)\n",
        "\n",
        "            mel_spectogram = tt.MelSpectrogram(sample_rate=samplerate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)(audio)\n",
        "\n",
        "            return audio,spectogram, mel_spectogram, genre_index\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        # List classes\n",
        "        fmt_str += '    Classes: {}\\n'.format(self.classes)\n",
        "        return fmt_str\n",
        "dataset = MusicDataset(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lKYr-sEfWzvp"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgc_yVIq8Cbq"
      },
      "source": [
        "Separamos en train y val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "-dr5Qhgk5sjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6eed6db-eece-4c51-fe53-aeab415511dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(790, 100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed);\n",
        "val_size = 100\n",
        "test_size = 100\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_ds),len(val_ds),len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "wBHjbBoo5sG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05fdbb3-4cb8-4d4a-f66c-776e0d1a671b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 20\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds,1, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "PkMIU-MAHTM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1a90a6-3add-4b1f-f0d4-79fd9f69fb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (256) may be set too high. Or, the value for `n_freqs` (256) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of waveform torch.Size([1, 110250]), sample rate with 22050, label is 0 \n"
          ]
        }
      ],
      "source": [
        "audio, spectogram, mel_spectogram, class_idx = train_dl.dataset[12]\n",
        "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(audio.size(),samplerate,class_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_VFArTDXy5L"
      },
      "source": [
        "### 3. Visualización de los archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIGGF2t9c-QM"
      },
      "outputs": [],
      "source": [
        "\n",
        "waveform,spectogram, mel_spectogram,label= dataset[0]\n",
        "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(waveform.size(),samplerate,label))\n",
        "# label = 9 es rock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjLl0uSfc_RI"
      },
      "outputs": [],
      "source": [
        "\n",
        "specgram=tt.Spectrogram()(waveform)\n",
        "print(\"shape of spectogram {}\".format(specgram.size()))\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.imshow(specgram.log2()[0,:,:].numpy(),cmap='magma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAVh9vlpdDHZ"
      },
      "outputs": [],
      "source": [
        "print(\"Waveform: {}\\n\".format(waveform))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWvDQtm6dE2j"
      },
      "source": [
        "Escuchamos el espectograma con la librería de audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvGzuWBFdGfB"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.display.Audio(waveform,rate=samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXxwYapDdHxy"
      },
      "outputs": [],
      "source": [
        "specgram.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAJncPPWXmFI"
      },
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed);\n",
        "val_size = 100\n",
        "test_size = 100\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_ds),len(val_ds),len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn7l6fspX6PA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 20\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds,1, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL9kGi-XMsz1"
      },
      "source": [
        "## **Ejercicio 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tASY5tzYMsz2"
      },
      "source": [
        "### Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "2_us-BjcMsz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9f37df-48d6-4e98-c52b-783e5d2e8217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([20, 1, 110250])\n",
            "Autoencoder(\n",
            "  (enc_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool1): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
            "  (enc_conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
            "  (enc_conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (dec_conv1): ConvTranspose1d(64, 64, kernel_size=(3,), stride=(3,))\n",
            "  (dec_conv2): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(5,))\n",
            "  (dec_conv3): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(1,))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, n_input=1, stride=16, n_channel=32):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder.\n",
        "        self.enc_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.pool1 = nn.MaxPool1d(5)\n",
        "        self.enc_conv2 = nn.Conv1d(n_channel, n_channel*2, kernel_size=3, padding=1, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(3)\n",
        "        self.enc_conv3 = nn.Conv1d(n_channel*2, n_channel*2, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        # Decoder.\n",
        "        self.dec_conv1 = nn.ConvTranspose1d(n_channel*2, n_channel*2, kernel_size=3, stride=3)\n",
        "        self.dec_conv2 = nn.ConvTranspose1d(n_channel*2, n_channel, kernel_size=3, stride=5)\n",
        "        self.dec_conv3 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=3, stride=1)\n",
        "\n",
        "    def forward_encoder(self, x):\n",
        "        # print(\"Encoder input shape:\", x.shape)\n",
        "        x = F.relu(self.enc_conv1(x))\n",
        "        # print(\"Encoder Conv1 shape:\", x.shape)\n",
        "        x = self.pool1(x)\n",
        "        # print(\"Encoder Pool1 shape:\", x.shape)\n",
        "        x = F.relu(self.enc_conv2(x))\n",
        "        # print(\"Encoder Conv2 shape:\", x.shape)\n",
        "        x = self.pool2(x)\n",
        "        # print(\"Encoder Pool2 shape:\", x.shape)\n",
        "        x = F.relu(self.enc_conv3(x))\n",
        "        # print(\"Encoder Conv3 shape:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def forward_decoder(self, x):\n",
        "        x = F.relu(self.dec_conv1(x))\n",
        "        # print(\"Decoder Conv1 shape:\", x.shape)\n",
        "        x = F.relu(self.dec_conv2(x))\n",
        "        # print(\"Decoder Conv2 shape:\", x.shape)\n",
        "        x = torch.sigmoid(self.dec_conv3(x))\n",
        "        # print(\"Decoder Conv3 shape:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_encoder(x)\n",
        "        x = self.forward_decoder(x)\n",
        "        return x\n",
        "\n",
        "# Assuming your audio has shape [batch_size, channels, sequence_length]\n",
        "audio_size = (batch_size, 1, 110250)  # Example shape\n",
        "model = Autoencoder(n_input=audio_size[1]).to(device)\n",
        "# Dummy input for testing\n",
        "dummy_input = torch.rand(audio_size).to(device)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.size())\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEJjPtyYyiR"
      },
      "source": [
        "#### Entrenamos autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfb_y2-pYyAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322b5f2a-1dc9-4f17-d99c-e589ba622970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/50], Train loss: 0.0976\n",
            "Epoch: [1/50], Valid loss: 0.0229\n",
            "Saving the best model at 0 epochs!\n",
            "Epoch: [2/50], Train loss: 0.0224\n",
            "Epoch: [2/50], Valid loss: 0.0207\n",
            "Saving the best model at 1 epochs!\n",
            "Epoch: [3/50], Train loss: 0.0184\n",
            "Epoch: [3/50], Valid loss: 0.0169\n",
            "Saving the best model at 2 epochs!\n",
            "Epoch: [4/50], Train loss: 0.0166\n",
            "Epoch: [4/50], Valid loss: 0.0161\n",
            "Saving the best model at 3 epochs!\n",
            "Epoch: [5/50], Train loss: 0.0156\n",
            "Epoch: [5/50], Valid loss: 0.0151\n",
            "Saving the best model at 4 epochs!\n",
            "Epoch: [6/50], Train loss: 0.0148\n",
            "Epoch: [6/50], Valid loss: 0.0147\n",
            "Saving the best model at 5 epochs!\n",
            "Epoch: [7/50], Train loss: 0.0143\n",
            "Epoch: [7/50], Valid loss: 0.0144\n",
            "Saving the best model at 6 epochs!\n",
            "Epoch: [8/50], Train loss: 0.0142\n",
            "Epoch: [8/50], Valid loss: 0.0141\n",
            "Saving the best model at 7 epochs!\n",
            "Epoch: [9/50], Train loss: 0.0140\n",
            "Epoch: [9/50], Valid loss: 0.0139\n",
            "Saving the best model at 8 epochs!\n",
            "Epoch: [10/50], Train loss: 0.0138\n",
            "Epoch: [10/50], Valid loss: 0.0138\n",
            "Saving the best model at 9 epochs!\n",
            "Epoch: [11/50], Train loss: 0.0137\n",
            "Epoch: [11/50], Valid loss: 0.0137\n",
            "Saving the best model at 10 epochs!\n",
            "Epoch: [12/50], Train loss: 0.0137\n",
            "Epoch: [12/50], Valid loss: 0.0136\n",
            "Saving the best model at 11 epochs!\n",
            "Epoch: [13/50], Train loss: 0.0136\n",
            "Epoch: [13/50], Valid loss: 0.0135\n",
            "Saving the best model at 12 epochs!\n",
            "Epoch: [14/50], Train loss: 0.0134\n",
            "Epoch: [14/50], Valid loss: 0.0134\n",
            "Saving the best model at 13 epochs!\n",
            "Epoch: [15/50], Train loss: 0.0135\n",
            "Epoch: [15/50], Valid loss: 0.0133\n",
            "Saving the best model at 14 epochs!\n",
            "Epoch: [16/50], Train loss: 0.0134\n",
            "Epoch: [16/50], Valid loss: 0.0134\n",
            "Epoch: [17/50], Train loss: 0.0132\n",
            "Epoch: [17/50], Valid loss: 0.0133\n",
            "Epoch: [18/50], Train loss: 0.0133\n",
            "Epoch: [18/50], Valid loss: 0.0133\n",
            "Epoch: [19/50], Train loss: 0.0132\n",
            "Epoch: [19/50], Valid loss: 0.0132\n",
            "Saving the best model at 18 epochs!\n",
            "Epoch: [20/50], Train loss: 0.0132\n",
            "Epoch: [20/50], Valid loss: 0.0131\n",
            "Saving the best model at 19 epochs!\n",
            "Epoch: [21/50], Train loss: 0.0132\n",
            "Epoch: [21/50], Valid loss: 0.0131\n",
            "Saving the best model at 20 epochs!\n",
            "Epoch: [22/50], Train loss: 0.0131\n",
            "Epoch: [22/50], Valid loss: 0.0131\n",
            "Saving the best model at 21 epochs!\n",
            "Epoch: [23/50], Train loss: 0.0132\n",
            "Epoch: [23/50], Valid loss: 0.0131\n",
            "Epoch: [24/50], Train loss: 0.0132\n",
            "Epoch: [24/50], Valid loss: 0.0131\n",
            "Epoch: [25/50], Train loss: 0.0132\n",
            "Epoch: [25/50], Valid loss: 0.0131\n",
            "Epoch: [26/50], Train loss: 0.0131\n",
            "Epoch: [26/50], Valid loss: 0.0131\n",
            "Epoch: [27/50], Train loss: 0.0131\n",
            "Epoch: [27/50], Valid loss: 0.0131\n",
            "Saving the best model at 26 epochs!\n",
            "Epoch: [28/50], Train loss: 0.0131\n",
            "Epoch: [28/50], Valid loss: 0.0131\n",
            "Epoch: [29/50], Train loss: 0.0130\n",
            "Epoch: [29/50], Valid loss: 0.0130\n",
            "Saving the best model at 28 epochs!\n",
            "Epoch: [30/50], Train loss: 0.0131\n",
            "Epoch: [30/50], Valid loss: 0.0130\n",
            "Saving the best model at 29 epochs!\n",
            "Epoch: [31/50], Train loss: 0.0130\n",
            "Epoch: [31/50], Valid loss: 0.0129\n",
            "Saving the best model at 30 epochs!\n",
            "Epoch: [32/50], Train loss: 0.0130\n",
            "Epoch: [32/50], Valid loss: 0.0129\n",
            "Epoch: [33/50], Train loss: 0.0130\n",
            "Epoch: [33/50], Valid loss: 0.0130\n",
            "Epoch: [34/50], Train loss: 0.0131\n",
            "Epoch: [34/50], Valid loss: 0.0130\n",
            "Epoch: [35/50], Train loss: 0.0129\n",
            "Epoch: [35/50], Valid loss: 0.0129\n",
            "Epoch: [36/50], Train loss: 0.0130\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "elif torch.backends.mps.is_available():\n",
        "    torch.mps.empty_cache()\n",
        "\n",
        "lr=0.0005\n",
        "num_epochs = 50\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect() #importante para ir liberando memoria ram\n",
        "valid_losses = []\n",
        "train_accs = []\n",
        "valid_accs = []\n",
        "\n",
        "log = False\n",
        "\n",
        "\n",
        "if log:\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=f\"{model.__class__.__name__}_lr={lr}_bs={batch_size}_epochs={num_epochs},{device}\",\n",
        "    )\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  # TRAIN\n",
        "  model.train()\n",
        "  for data in train_dl:\n",
        "      audio,spectogram, mel_spectogram, genre_index = data\n",
        "      audio = audio.to(device)\n",
        "      # print(audio.shape)\n",
        "\n",
        "      # Forward\n",
        "      out = model(audio)\n",
        "      # ?\n",
        "      loss = loss_function(out.squeeze(), audio.squeeze())\n",
        "\n",
        "      # Backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "      del spectogram #importante para ir liberando memoria ram\n",
        "      del genre_index #importante para ir liberando memoria ram\n",
        "      del audio #importante para ir liberando memoria ram\n",
        "      del loss #importante para ir liberando memoria ram\n",
        "      del out  #importante para ir liberando memoria ram\n",
        "      torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "      gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "  print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(train_loss)))\n",
        "\n",
        "  # VALIDATION\n",
        "  model.eval()\n",
        "  correct =0\n",
        "  for data in train_dl:\n",
        "      audio,spectogram, mel_spectogram, genre_index = data\n",
        "      audio = audio.to(device)\n",
        "      # print(audio.shape)\n",
        "\n",
        "      # Forward\n",
        "      out = model(audio)\n",
        "      # ?\n",
        "      loss = loss_function(out.squeeze(), audio.squeeze())\n",
        "\n",
        "      val_loss.append(loss.item())\n",
        "\n",
        "      del spectogram #importante para ir liberando memoria ram\n",
        "      del genre_index #importante para ir liberando memoria ram\n",
        "      del audio #importante para ir liberando memoria ram\n",
        "      del loss #importante para ir liberando memoria ram\n",
        "      del out  #importante para ir liberando memoria ram\n",
        "      torch.cuda.empty_cache()  #importante para ir liberando memoria ram\n",
        "      gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "  valid_loss = np.mean(val_loss)\n",
        "  print('Epoch: [%d/%d], Valid loss: %.4f' % (epoch+1, num_epochs, valid_loss))\n",
        "\n",
        "  # Save model\n",
        "  valid_losses.append(valid_loss.item())\n",
        "  if np.argmin(valid_losses) == epoch:\n",
        "      print('Saving the best model at %d epochs!' % epoch)\n",
        "      torch.save(model.state_dict(), 'best_model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "S = torch.load('best_model.ckpt')\n",
        "model.load_state_dict(S)\n",
        "print('loaded!')\n",
        "\n",
        "# Run evaluation\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for audio,spectogram, mel_spectogram, genre_index in test_dl:\n",
        "        audio = audio.to(device)\n",
        "        genre_index = genre_index.to(device)\n",
        "\n",
        "        out = model(audio)\n",
        "\n",
        "        pred= out.argmax(dim=-1).flatten()\n",
        "        # append labels and predictions\n",
        "        correct += pred.eq(genre_index).sum().item()\n",
        "        y_true.extend(genre_index)\n",
        "        y_pred.extend(pred)\n",
        "\n",
        "accuracy =correct/ len(test_dl.dataset)\n",
        "print('Epoch: [%d/%d], Valid loss: %.4f' % (epoch+1, num_epochs, valid_loss))\n"
      ],
      "metadata": {
        "id": "A0LuhgGExKYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original = test_dl.dataset[12]\n",
        "print(original)\n",
        "audio = original[0]\n",
        "idnex = original[3]\n",
        "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(audio.size(),samplerate,idnex))"
      ],
      "metadata": {
        "id": "x14VFDF8v5ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.display.Audio(audio, rate=22050)"
      ],
      "metadata": {
        "id": "3TuWm65mwwS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio = audio.to(device)\n",
        "print(audio.shape)\n",
        "\n",
        "out = model(audio)"
      ],
      "metadata": {
        "id": "E-UBZnlfwqcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.display.Audio(out.detach().cpu(), rate=22050)"
      ],
      "metadata": {
        "id": "zuhjisIxwmeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "#original\n",
        "waveform, spectogram, mel_spectogram, label = next(iter(test_dl))\n",
        "audio = waveform.to(device)\n",
        "spec = spectogram.to(device)\n",
        "#reconstruccion\n",
        "rec_audio = model(audio).to('cpu').squeeze(0).detach()\n",
        "rec_spec = tt.Spectrogram(n_fft=1024)(rec_audio)\n",
        "\n",
        "print(\"spectogram shape: \", spectogram.shape)\n",
        "print(\"reconstructed spectogram shape: \", rec_spec.shape)\n",
        "print()\n",
        "hop_length=432\n",
        "n_mels=256\n",
        "n_fft = 2*(n_mels - 1)\n",
        "\n",
        "rec_mel_spec = tt.MelSpectrogram(sample_rate=samplerate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)(rec_audio)\n",
        "mel_spect = mel_spectogram.to(device)\n",
        "\n",
        "print(\"mel_spectogram shape: \", mel_spect.shape)\n",
        "print(\"reconstructed mel_spectogram shape: \", rec_mel_spec.shape)\n",
        "\n",
        "# rec_audio = rec_audio.sAqueeze(0)\n",
        "\n",
        "print(\"waveform shape: \", waveform.shape)\n",
        "print(\"waveform new shape: \", rec_audio.shape)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 8))\n",
        "\n",
        "axs[0, 0].plot(audio.squeeze().detach().cpu().numpy())\n",
        "axs[0, 0].set_title(\"Original Waveform\")\n",
        "\n",
        "axs[0, 1].plot(rec_audio.numpy())\n",
        "axs[0, 1].set_title(\"Reconstruido Waveform\")\n",
        "\n",
        "axs[1, 0].imshow(spec.log2().squeeze().detach().cpu().numpy(), cmap='magma')\n",
        "axs[1, 0].set_title(\"Original Spectogram\")\n",
        "\n",
        "axs[1, 1].imshow(rec_spec.log2().squeeze().detach().cpu().numpy(), cmap='magma')\n",
        "axs[1, 1].set_title(\"Reconstruido Spectogram\")\n",
        "\n",
        "print(\"Audio Original\")\n",
        "IPython.display.Audio(audio, rate=samplerate)\n",
        "print(\"Audio Reconstruido\")\n",
        "IPython.display.Audio(rec_audio, rate=samplerate)"
      ],
      "metadata": {
        "id": "NM5yTG3TFTV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQHB9WrNEbk"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gYWaW8hNHy9"
      },
      "outputs": [],
      "source": [
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "        return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        ##print(x.shape, skip.shape)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "    # def forward(self, inputs, skip):\n",
        "    #     x = self.up(inputs)\n",
        "    #     # Ajuste de dimensiones\n",
        "    #     diffY = skip.size()[2] - x.size()[2]\n",
        "    #     diffX = skip.size()[3] - x.size()[3]\n",
        "\n",
        "    #     x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
        "    #                   diffY // 2, diffY - diffY // 2])\n",
        "    #     x = torch.cat([x, skip], axis=1)\n",
        "    #     x = self.conv(x)\n",
        "    #     return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        self.e1 = encoder_block(1, 16)\n",
        "        self.e2 = encoder_block(16, 32)\n",
        "        self.e3 = encoder_block(32, 64)\n",
        "        self.e4 = encoder_block(64, 128)\n",
        "\n",
        "        # self.e1 = encoder_block(256, 128)\n",
        "        # self.e2 = encoder_block(128, 64)\n",
        "        # self.e3 = encoder_block(64, 32)\n",
        "        # self.e4 = encoder_block(32, 16)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        #self.b = conv_block(16, 32)\n",
        "        self.b = conv_block(128, 256)\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        self.d1 = decoder_block(256, 128)\n",
        "        self.d2 = decoder_block(128, 64)\n",
        "        self.d3 = decoder_block(64, 32)\n",
        "        self.d4 = decoder_block(32, 16)\n",
        "\n",
        "        # self.d1 = decoder_block(32, 64)\n",
        "        # self.d2 = decoder_block(64, 128)\n",
        "        # self.d3 = decoder_block(128, 256)\n",
        "        # self.d4 = decoder_block(256, 512)\n",
        "\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        self.outputs = nn.Conv2d(16, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    # def encode(self, inputs):\n",
        "    #     \"\"\" Encoder \"\"\"\n",
        "    #     #print(\"I\", inputs.shape)\n",
        "    #     s1, p1 = self.e1(inputs)\n",
        "    #     #print(\"E1\", s1.shape, p1.shape)\n",
        "    #     s2, p2 = self.e2(p1)\n",
        "    #     #print(\"E2\", s2.shape, p2.shape)\n",
        "    #     s3, p3 = self.e3(p2)\n",
        "    #     #print(\"E3\", s3.shape, p3.shape)\n",
        "    #     s4, p4 = self.e4(p3)\n",
        "    #     #print(\"E4\", s4.shape, p4.shape)\n",
        "\n",
        "\n",
        "    #     return p4, (s1, s2, s3, s4)\n",
        "\n",
        "    # def decode(self, inputs, skips):\n",
        "    #     \"\"\" Decoder \"\"\"\n",
        "    #     #print(\"D\", inputs.shape, skips[3].shape)\n",
        "    #     d1 = self.d1(inputs, skips[3])\n",
        "    #     #print(\"D1\", d1.shape, skips[2].shape)\n",
        "    #     d2 = self.d2(d1, skips[2])\n",
        "    #     #print(\"D2\", d2.shape, skips[1].shape)\n",
        "    #     d3 = self.d3(d2, skips[1])\n",
        "    #     #print(\"D3\", d3.shape, skips[0].shape)\n",
        "    #     d4 = self.d4(d3, skips[0])\n",
        "    #     #print(\"D4\", d4.shape)\n",
        "\n",
        "\n",
        "    #     return d4\n",
        "    # def forward(self, inputs):\n",
        "    #     \"\"\" Encoder \"\"\"\n",
        "    #     p4, (s1, s2, s3, s4) = self.encode(inputs)\n",
        "    #     \"\"\" Bottleneck \"\"\"\n",
        "    #     b = self.b(p4)\n",
        "    #     \"\"\" Decoder \"\"\"\n",
        "    #     d4 = self.decode(b, (s1, s2, s3, s4))\n",
        "    #     \"\"\" Classifier \"\"\"\n",
        "    #     outputs = self.outputs(d4)\n",
        "    #     # #print(outputs.shape)\n",
        "    #     return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        s1, p1 = self.e1(inputs)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        b = self.b(p4)\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        outputs = self.outputs(d4)\n",
        "        # #print(outputs.shape)\n",
        "        return outputs\n",
        "\n",
        "model = UNet().to(device)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "n = count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGV5RsFGWksV"
      },
      "source": [
        "#### entrenamos y evaluamos UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heMxKhxVOvWL"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "elif torch.backends.mps.is_available():\n",
        "    torch.mps.empty_cache()\n",
        "\n",
        "gc.collect() # importante para ir liberando memoria ram\n",
        "\n",
        "val_loss = 0\n",
        "val_acc = 0\n",
        "train_acc = 0\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "# Loss function is pixel wise\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "\n",
        "valid_accs = []\n",
        "num_epochs = 30\n",
        "\n",
        "log = False\n",
        "\n",
        "\n",
        "if log:\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=f\"{model.__class__.__name__}_lr={lr}_bs={batch_size}_epochs={num_epochs},{device}\",\n",
        "    )\n",
        "\n",
        "# IPython clear cell output\n",
        "# IPython.display.clear_output()\n",
        "\n",
        "iterator = tqdm(range(num_epochs), total=num_epochs, desc=\"Epoch\")\n",
        "\n",
        "for epoch in iterator:\n",
        "    train_losses_itter = []\n",
        "\n",
        "    total = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    # Train\n",
        "    model.train()\n",
        "    for wav, _, spectogram, genre_index in train_dl:\n",
        "        #print(spectogram.shape)\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        spectogram = spectogram.to(device)#.squeeze(1)\n",
        "        #print(spectogram.shape)\n",
        "\n",
        "        # Spectogram is Batched [10, 1, 256, 256]\n",
        "        # Squeeze to [10, 256, 256]\n",
        "        # Unsqueeze to [10, 256, 256, 1]\n",
        "        #spectogram = spectogram.squeeze(1).unsqueeze(3)\n",
        "\n",
        "        #print(spectogram.shape)\n",
        "        #genre_index = torch.as_tensor(genre_index).to(device)\n",
        "\n",
        "        # Forward\n",
        "        out = model(spectogram)\n",
        "\n",
        "        # MODEL UNET, loss function\n",
        "        loss = loss_function(out.squeeze().reshape(spectogram.shape), spectogram)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses_itter.append(loss.item())\n",
        "\n",
        "        pred = out.argmax(dim=-1).flatten()\n",
        "        train_correct = 0\n",
        "\n",
        "        total += len(pred)\n",
        "        train_acc = 100 * train_correct / total\n",
        "\n",
        "        del spectogram #importante para ir liberando memoria ram\n",
        "        del genre_index #importante para ir liberando memoria ram\n",
        "        del loss #importante para ir liberando memoria ram\n",
        "        del out  #importante para ir liberando memoria ram\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        elif torch.backends.mps.is_available():\n",
        "            torch.mps.empty_cache()\n",
        "\n",
        "        gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "\n",
        "        iterator.set_postfix_str(\n",
        "            {\n",
        "                \"Train loss\": round(np.mean(train_losses_itter), 4),\n",
        "                \"Train accuracy\": round(train_acc, 4),\n",
        "                \"Valid loss\": round(val_loss, 4),\n",
        "                \"Valid accuracy\": round(val_acc, 4),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_losses.append(np.mean(train_losses_itter))\n",
        "        valid_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        valid_accs.append(val_acc)\n",
        "\n",
        "    #print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    val_losses_itter = []\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for wav, _, spectogram, genre_index in valid_dl:\n",
        "        #print(wav, genre, index)\n",
        "        spectogram = spectogram.to(device)\n",
        "        #spectogram = spectogram.squeeze(1).unsqueeze(3)\n",
        "\n",
        "        out = model(spectogram)\n",
        "\n",
        "        loss = loss_function(out.squeeze().reshape(spectogram.shape), spectogram)\n",
        "\n",
        "        val_losses_itter.append(loss.item())\n",
        "\n",
        "        pred = out.argmax(dim=-1).flatten()\n",
        "\n",
        "        correct = 0\n",
        "\n",
        "        del spectogram\n",
        "        del genre_index\n",
        "        del loss\n",
        "        del out\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        elif torch.backends.mps.is_available():\n",
        "            torch.mps.empty_cache()\n",
        "\n",
        "        gc.collect() #importante para ir liberando memoria ram\n",
        "\n",
        "    #accuracy = correct / len(valid_dl.dataset)\n",
        "    val_acc = 100 * correct / len(valid_dl.dataset)\n",
        "    val_loss = np.mean(val_losses_itter)\n",
        "\n",
        "    #print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' % (epoch+1, num_epochs, val_loss, accuracy))\n",
        "\n",
        "\n",
        "    iterator.set_postfix_str(\n",
        "        {\n",
        "            \"Train loss\": round(np.mean(train_losses)),\n",
        "            \"Train accuracy\": round(train_acc),\n",
        "            \"Valid loss\": round(val_loss),\n",
        "            \"Valid accuracy\": round(val_acc),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if log:\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"Train loss\": np.mean(train_losses),\n",
        "                \"Train accuracy\": train_acc,\n",
        "                \"Valid loss\": val_loss,\n",
        "                \"Valid accuracy\": val_acc,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "if log:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ-v8FsqP5g0"
      },
      "outputs": [],
      "source": [
        "waveform, spectogram, mel_spectogram, label = next(iter(train_dl))\n",
        "spec = mel_spectogram.to(device)\n",
        "model.eval()\n",
        "out = model(spec)\n",
        "out.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJkX8G62hXGy"
      },
      "outputs": [],
      "source": [
        "# Waveform from mel spectogram\n",
        "waveform, spectogram, mel_spectogram, label = dataset[np.random.randint(len(dataset))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zunld4brQSar"
      },
      "outputs": [],
      "source": [
        "spec = mel_spectogram.to(device)\n",
        "model.eval()\n",
        "rec_spec = model(spec).to('cpu').squeeze(0).detach()\n",
        "mel_spectogram = mel_spectogram.squeeze(0).detach().cpu()\n",
        "print(\"waveform\", waveform.shape)\n",
        "print(\"spectogram\", spectogram.shape)\n",
        "print(\"mel_spectogram\", mel_spectogram.shape)\n",
        "\n",
        "print(\"rec_spec\", rec_spec.shape)\n",
        "\n",
        "hop_length=432\n",
        "n_mels=256\n",
        "n_fft = 2*(n_mels - 1)\n",
        "\n",
        "waveform_out = torchaudio.transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)(rec_spec)\n",
        "waveform = waveform.squeeze(0)\n",
        "print(\"waveform_out\", waveform_out.shape)\n",
        "print(\"waveform\", waveform.shape)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 5))\n",
        "\n",
        "axs[0, 0].plot(waveform.t().numpy())\n",
        "axs[0, 0].set_title(\"Original Waveform\")\n",
        "\n",
        "axs[0, 1].plot(waveform_out.t().numpy())\n",
        "axs[0, 1].set_title(\"Reconstructed Waveform\")\n",
        "\n",
        "axs[1, 0].imshow(mel_spectogram.log2()[0,:,:].numpy(), cmap='magma')\n",
        "axs[1, 0].set_title(\"Original Spectogram\")\n",
        "\n",
        "axs[1, 1].imshow(rec_spec.squeeze().detach().cpu().numpy(), cmap='magma')\n",
        "axs[1, 1].set_title(\"Reconstructed Spectogram\")\n",
        "\n",
        "#IPython.display.Audio(waveform_out.squeeze(), rate=22050)\n",
        "\n",
        "print(waveform.shape, waveform_out.shape, mel_spectogram.shape, rec_spec.shape)\n",
        "IPython.display.Audio(waveform, rate=samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9siS0tzMsz3"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(waveform_out, rate=samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eaHFLkhQZul"
      },
      "outputs": [],
      "source": [
        "# Convert mel_spectogram to waveform\n",
        "waveform, spectogram, mel_spectogram, label = dataset[np.random.randint(len(dataset))]\n",
        "\n",
        "og_waveform = waveform\n",
        "\n",
        "hop_length=432\n",
        "n_mels=256\n",
        "n_fft = 2*(n_mels - 1)\n",
        "\n",
        "f_spec = torchaudio.transforms.MelSpectrogram(sample_rate=samplerate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "\n",
        "griffin_lim = torchaudio.transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "spec = f_spec(waveform)\n",
        "\n",
        "rec_waveform = griffin_lim(spec)\n",
        "\n",
        "print(\"Waveform: {}\\n\".format(waveform))\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 5))\n",
        "\n",
        "axs[0, 0].plot(waveform.t().numpy())\n",
        "axs[0, 0].set_title(\"Original Waveform\")\n",
        "\n",
        "axs[0, 1].plot(rec_waveform.t().numpy())\n",
        "axs[0, 1].set_title(\"Reconstructed Waveform\")\n",
        "\n",
        "axs[1, 0].imshow(spec.log2()[0,:,:].numpy(), cmap='magma')\n",
        "axs[1, 0].set_title(\"Original Spectogram\")\n",
        "\n",
        "axs[1, 1].imshow(f_spec(rec_waveform).log2()[0,:,:].numpy(), cmap='magma')\n",
        "axs[1, 1].set_title(\"Reconstructed Spectogram\")\n",
        "\n",
        "print(spec.shape)\n",
        "print(waveform.shape)\n",
        "print(rec_waveform.shape)\n",
        "\n",
        "IPython.display.Audio(torch.cat([waveform, rec_waveform], axis=1), rate=22050)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTIMsUu4QdQE"
      },
      "outputs": [],
      "source": [
        "ex_spec = f_spec(og_waveform)\n",
        "new_spec = f_spec(rec_waveform)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "axs[0].imshow(ex_spec.log2()[0,:,:].numpy(),cmap='magma')\n",
        "axs[0].set_title(\"Original Spectogram\")\n",
        "\n",
        "axs[1].imshow(new_spec.log2()[0,:,:].numpy(),cmap='magma')\n",
        "axs[1].set_title(\"Reconstructed Spectogram\")\n",
        "\n",
        "ex_spec.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SUZxclEkTOcS",
        "oU5G8mTE-5zM"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}